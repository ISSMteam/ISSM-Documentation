% Just The Docs Front Matter
% title: Quantifications of Margins and Uncertainties (QMU) with Dakota
% parent: Advanced Features
% has_children: false
% has_toc: false

\subsection{Quantifications of Margins and Uncertainties (QMU) with Dakota} \label{sec:using-issm-advanced-qmu-dakota}
\subsubsection{Physical basis}
The methods for Quantification of Margins and Uncertainties (QMU) are based on the Design Analysis Kit for Optimization and Terascale Applications (Dakota) software \citep{Eldred2008}, which is embedded within ISSM \citep{Larour2012b,Larour2012a}.  Available Dakota analyses include sensitivity and sampling analyses, which we respectfully rely on to: 1) understand the sensitivity of model diagnostics to local variations in model fields and 2) identify how variations in model fields impact uncertainty in model diagnostics.  Diagnostics of interest include ice volume, maximum velocity, and mass flux across user-specified profiles.

\paragraph{Mesh Partitioning}
QMU analyses are carried out on partitions of the model domain. Each partition consists of a collection of vertices.  The ISSM partitioner is versatile. For example, the partitioner can assign one vertex for each partition (linear partitioning); the same number of vertices per partition (unweighted partitioning); or it can weight partitions by a specified amount (equal-area by default - to remove area-specific dependencies).  Advanced partitioning is accomplished using the Chaco Software for Partitioning Graphs \citep{Hendrickson1995}, prior to setting up the model parameters for QMU analysis.

\paragraph{Sensitivity}
Sensitivity, or local reliability, analysis computes the local derivative of diagnostics with respect to model inputs. It is used to assess the spatial distribution of this derivative, for the purpose of spatially ranking the influence of various inputs.

Given a response $r$ that is a function of multiple variables $x_i$ in a local reliability analysis \cite{Coleman1999}, we have:
\begin{equation}
	r=r(x_1,x_2,...,x_n)
\end{equation}
where the sensitivities are defined as:
\begin{equation}
	\theta_i=\frac{\delta r}{\delta x_i}
\end{equation}

If each of the variables is independent, the error propagation equation defines the variance of $r$ as:
\begin{equation}
	\sigma_r^2=\sum_{i=1}^n\theta_i^2 \sigma_i^2
\end{equation}
where $\sigma_i$ is the standard deviation of $x_i$ and $\sigma_r$ is the standard deviation of $r$.

\emph{Importance factors} for each $x_i$ are determined by dividing the error propagation equation by $\sigma$\textsubscript{r}\textsuperscript{2}. Note that the mean of the response is taken to be the response for the nominal value of each variable $x_i$.

Sensitivities are computed from the function evaluations using finite differences. The finite difference step size is user-defined by a parameter in the ISSM model. This analysis imposes the finite-difference step size as a small perturbation to $x_i$. The resulting sensitivities quantify
how the location of errors impact a specified model diagnostic ($r$).

First, Dakota calls one ISSM model solve for an un-perturbed control simulation. Then, for every $x_i$, ISSM perturbs each partition one at a time, and calls an ISSM solve for each. At every partition, $p$, a resulting sensitivity, $\theta_i(p)$ is assigned. Each $\theta_i$ (defined above) is dependent on how much the outcome diverges from the control run. The result is a spatial mapping of sensitivities and importance factors of $r$ for every $x_i$. For Transient solves, sensitivities are determined only at the completion of a forward run.

\emph{Method inputs}: $\sigma_i$ for each $x_i$ at every partition and the finite difference step

\emph{Method outputs}: sensitivities ($\theta_i$) and importance factors for each $x_i$ at every partition

\paragraph{Sampling}
Sampling analysis quantifies how input errors propagate through a model to impact a specified diagnostic, $r$. It a Monte-Carlo-style method that relies upon repeated execution (samples) of the same model, where input variables are perturbed by different amounts at each partition for each individual run. Resulting statistics (mean, standard deviations, cumulative distribution functions) are calculated after the specified number of samples are run.

For a particular sample, every $x_i$ is perturbed by a different amount at each partition. Input values are perturbed randomly, per partition, within a prescribed range (described by a statistical distribution, e.g. normal or uniform). Once the variables are perturbed, the ISSM model solve is called.

\emph{Distributions}: A normal distribution for a particular partition is fully described by an average, $\mu_i$, and a standard deviation, $\sigma_i$. By definition, normal distributions cluster around $\mu_i$ and decrease towards the tails, in a Gaussian bell curve ranging from $\mu_i\pm 3\sigma_i$. A uniform distribution places greater emphasis on values closer to the tails, where probability of occurrence is equal for any given value within a specified minimum and maximum value.

If a user chooses so, any $x_i$ can be treated as a scaled value. In this case, the distribution definitions are given in percentages, relative to a $\mu_i$ value of 1.

For example, at the beginning of a particular sample for a scaled $x_i$, Dakota chooses a random percentage perturbation $P_i(p)$ at each partition $p$. The value of the random percentage will fall within the defined error distribution, and the new value of $x_i$ for duration of this sample run is perturbed by $x_iP_i(p)$. The generation algorithm for $P_i(p)$ is user-specified (e.g. Monte-Carlo or LHS \citep{Swiler2004}).

In the case where the user wants to sample $n$ variables at the same time, a $P_i(p)$ is chosen separately for each $x_i$ before a particular sample run. Resulting statistics reflect the combined effects of the errors due to $x_1,x_2,...,x_n$.

For Transient simulations, $P_i(p)$ remains constant for the duration of a particular sample run. Note that statistics are determined only at the completion of each forward run.

\emph{Method inputs}: The number of samples to be run and for every $x_i$, a definition of error distribution (error ranges may vary spatially by partition)

\emph{Method outputs}: For $r$, mean, standard deviations, and cumulative distribution functions resulting from errors due to $x_1,x_2,...,x_n$

\subsubsection{Model parameters}
The parameters relevant to uncertainty quantification can be displayed by typing:
\begin{lstlisting}
>> md.qmu
\end{lstlisting}

\begin{itemize}
	\item \lstinlinebg|md.qmu.isdakota|: 1 to activate qmu analysis, or else 0
	\item \lstinlinebg|md.qmu.variables|: arrays of each \lstinlinebg|variable| class
	\item \lstinlinebg|md.qmu.responses|: arrays of each \lstinlinebg|diagnostics| class
	\item \lstinlinebg|md.qmu.numberofresponses|: number of responses
	\item \lstinlinebg|md.qmu.params|: array of method-independent parameters
	\item \lstinlinebg|md.qmu.results|: holder class for information from dakota result files
	\item \lstinlinebg|md.qmu.partition|: user provided, the partition each vertex belongs to
	\item \lstinlinebg|md.qmu.numberofpartitions|: number of partitions
	\item \lstinlinebg|md.qmu.variabledescriptors|: list of user-defined descriptors for variables
	\item \lstinlinebg|md.qmu.responsedescriptors|: list of user-defined descriptors for diagnostics
	\item \lstinlinebg|md.qmu.method|: array of \lstinlinebg|dakota_method| class
	\item \lstinlinebg|md.qmu.mass_flux_profile_directory|: directory for mass flux profiles
	\item \lstinlinebg|md.qmu.mass_flux_profiles|: list of \lstinlinebg|mass_flux| profiles
	\item \lstinlinebg|md.qmu.mass_flux_segments|: used by \lstinlinebg|process_qmu_response_data| to store processed profiles
	\item \lstinlinebg|md.qmu.adjacency|: adjacency matrix from connectivity table, partitioner computes it by default
	\item \lstinlinebg|md.qmu.vertex_weight|: weight for each vertex, partitioner sets it from connectivity by default
\end{itemize}

\subsubsection{Building the Chaco and Dakota packages}
In order to run Dakota with ISSM, you must compile and install the external package Dakota (\lstinlinebg|${ISSM_DIR}/externalpackages/dakota|). In addition, for complex partitioning (more than one vertex per partition), you must compile and install the external package Chaco (\lstinlinebg|${ISSM_DIR}/externalpackages/chaco|).

In addition, your configure script should include the following:
\begin{lstlisting}
--with-chaco-dir=${ISSM_DIR}/externalpackages/chaco/install \
--with-dakota-dir=${ISSM_DIR}/externalpackages}/dakota/install \
\end{lstlisting}

More recent versions of Dakota also require the external package Boost (\lstinlinebg|${ISSM_DIR}/externalpackages/boost|). If installed, it should also be added to your configure script:
\begin{lstlisting}
--with-boost-dir=${ISSM_DIR}/externalpackages/boost/install/ \
\end{lstlisting}

\subsubsection{Partitioning a Mesh}
To partition your mesh using Chaco, use the following commands:
\begin{lstlisting}
>> md.qmu.numberofpartitions = 1000; % Note: Chaco can crash if too large
>> md = partitioner(md, 'package', 'chaco', 'npart', md.qmu.numberofpartitions, 'weighting', 'on');
%weighting on for weighted partitioning (equal-area by default), off for equal vertex partitioning
>> md.qmu.partition = md.qmu.partition - 1; %With chaco, partition numbers must be adjusted by 1
\end{lstlisting}
or, for a 1-to-1 mapping of vertices to partitions:
\begin{lstlisting}
>> md.qmu.numberofpartitions = md.mesh.number_of_vertices;
>> md = partitioner(md, 'package', 'linear');
\end{lstlisting}

\subsubsection{Setting up the QMU}
\paragraph{For sensitivity}
\begin{lstlisting}
>> md.qmu.method = dakota_method('nond_l');
\end{lstlisting}
This sets the method to local reliability (sensitivity). Other sensitivity settings:
\begin{lstlisting}
>> md.qmu.params.fd_gradient_step_size = '0.1'; %finite difference step size, 0.001 by default
\end{lstlisting}

\paragraph{For sampling}
\begin{lstlisting}
>> md.qmu.method = dakota_method('nond_samp');
>> md.qmu.method(end) = ...
dmeth_params_set(md.qmu.method(end), 'seed', 1234, 'samples', 500, 'sample_type', 'lhs');
\end{lstlisting}
where \lstinlinebg|'seed'| is used for reproducibility of results and \lstinlinebg|'samples'| is the number of samples requested.

Other sampling settings:
\begin{lstlisting}
>> md.qmu.params.tabular_graphics_data = true; %Output all the information needed to create histograms of results
\end{lstlisting}

\paragraph{Other simple default settings for both sampling and sensitivity}
\begin{lstlisting}
>> md.qmu.params.evaluation_concurrency = 1;
>> md.qmu.params.analysis_driver = '';
>> md.qmu.params.analysis_components = '';
>> md.qmu.params.direct = true;
\end{lstlisting}

\subsubsection{Setting your QMU variables}
Example: Here, the input of interest is md.friction.coefficient, scaled, with error defined as a normal distribution with a mean of 1 and a standard deviation of 10\%:
\begin{lstlisting}
>> md.qmu.variables.drag_coefficient = normal_uncertain('scaled_FrictionCoefficient', 1, 0.1);
\end{lstlisting}

This sets the standard deviation to a constant value at every partition. After it is initialized as above, the standard deviation can be set manually, so that it varies spatially by partition:
\begin{lstlisting}
md.qmu.variables.drag_coefficient.stddev = uncertainty_on_partition;
\end{lstlisting}

See also:
\begin{lstlisting}
>> help normal_uncertain
>> help uniform_uncertain
>> help AreaAverageOntoPartition
\end{lstlisting}

\subsubsection{Setting your diagnostics}
Example: Here, diagnostics of interest are (1) maximum velocity and (2) mass flux through two different gates. Mass flux gates are defined by the ARGUS files \lstinlinebg|'../Exp/MassFlux1.exp'| and \lstinlinebg|'../Exp/MassFlux2.exp'|:
\begin{lstlisting}
%responses
md.qmu.responses.MaxVel = response_function('MaxVel', [], [0.01 0.25 0.5 0.75 0.99]);
md.qmu.responses.MassFlux1 = response_function('indexed_MassFlux_1', [], [0.01 0.25 0.5 0.75 0.99]);
md.qmu.responses.MassFlux2 = response_function('indexed_MassFlux_2', [], [0.01 0.25 0.5 0.75 0.99]);

%mass flux profiles
md.qmu.mass_flux_profiles = {'../Exp/MassFlux1.exp', '../Exp/MassFlux2.exp'};
md.qmu.mass_flux_profile_directory = pwd;
\end{lstlisting}

For more options see:
\begin{lstlisting}
>> help response_function
\end{lstlisting}

\subsubsection{Running a simulation}
Note: You must set your stress balance tolerance to $10^{-5}$ or smaller in order to avoid the accumulation of numerical residuals between consecutive samples:
\begin{lstlisting}
>> md.stressbalance.restol = 10^-5;
\end{lstlisting}

To initiate the analysis of choice, use the following commands:
\begin{lstlisting}
>> md.qmu.isdakota = 1;
>> md = solve(md, 'Masstransport');
\end{lstlisting}
The first argument is the model, the second is the nature of the simulation one wants to run.

\clearpage % Make sure all figures are placed before next section
